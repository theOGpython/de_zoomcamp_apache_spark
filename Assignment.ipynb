{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081cac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540441ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/GBENGA/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/02 16:16:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23adab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20a0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-01 15:38:08--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.132.17\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.132.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  29.2MB/s    in 23s     \n",
      "\n",
      "2022-03-01 15:38:32 (29.9 MB/s) - ‘fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c87b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11613943 fhvhv_tripdata_2021-02.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318b6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 101 fhvhv_tripdata_2021-02.csv > 02_head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0d3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49933963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv_pd = pd.read_csv('02_head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fceccc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00680ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_fhvhv_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d31296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2a1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num',types.StringType(),True),\n",
    "    types.StructField('dispatching_base_num',types.StringType(),True),\n",
    "    types.StructField('pickup_datetime',types.TimestampType(),True),\n",
    "    types.StructField('dropoff_datetime',types.TimestampType(),True),\n",
    "    types.StructField('PULocationID',types.IntegerType(),True),\n",
    "    types.StructField('DOLocationID',types.IntegerType(),True),\n",
    "    types.StructField('SR_Flag',types.DoubleType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1cb10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379ac122",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3905366/2134291807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fhvhv/2021/02/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.repartition(24).write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631a336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a654387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2021-02-02 22:06:25|2021-02-02 22:13:17|         126|         147|   null|\n",
      "|           HV0003|              B02836|2021-02-05 13:37:18|2021-02-05 13:52:28|          85|          72|   null|\n",
      "|           HV0003|              B02872|2021-02-05 20:59:47|2021-02-05 21:19:44|         229|         261|   null|\n",
      "|           HV0005|              B02510|2021-02-04 14:19:42|2021-02-04 14:49:20|         260|         160|   null|\n",
      "|           HV0003|              B02682|2021-02-05 08:07:49|2021-02-05 09:00:13|         140|          72|   null|\n",
      "|           HV0003|              B02764|2021-02-05 19:18:08|2021-02-05 19:31:58|          51|         213|   null|\n",
      "|           HV0003|              B02765|2021-02-02 08:51:29|2021-02-02 09:06:17|         143|         137|   null|\n",
      "|           HV0003|              B02765|2021-02-01 14:21:29|2021-02-01 14:23:27|         260|         260|   null|\n",
      "|           HV0003|              B02876|2021-02-06 14:33:24|2021-02-06 14:44:59|         167|         169|   null|\n",
      "|           HV0003|              B02872|2021-02-06 11:02:15|2021-02-06 11:22:50|         233|         158|   null|\n",
      "|           HV0003|              B02875|2021-02-05 19:18:33|2021-02-05 19:34:52|          60|           3|   null|\n",
      "|           HV0005|              B02510|2021-02-03 19:55:35|2021-02-03 20:01:57|         125|          68|   null|\n",
      "|           HV0003|              B02764|2021-02-02 06:56:41|2021-02-02 07:02:38|          68|         246|   null|\n",
      "|           HV0003|              B02764|2021-02-06 10:06:52|2021-02-06 10:20:25|         249|          45|   null|\n",
      "|           HV0003|              B02875|2021-02-06 12:44:05|2021-02-06 12:58:51|          49|          40|   null|\n",
      "|           HV0003|              B02764|2021-02-03 06:40:44|2021-02-03 06:47:45|         198|         157|   null|\n",
      "|           HV0003|              B02682|2021-02-01 08:33:06|2021-02-01 08:59:47|         166|         137|   null|\n",
      "|           HV0003|              B02835|2021-02-04 12:07:55|2021-02-04 12:19:19|         236|         262|   null|\n",
      "|           HV0003|              B02872|2021-02-05 15:30:55|2021-02-05 16:27:11|         233|           1|   null|\n",
      "|           HV0003|              B02866|2021-02-05 14:44:38|2021-02-05 15:13:48|         257|         231|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62be40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68fd7cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_count_records = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*)\n",
    "FROM fhvhv\n",
    "WHERE date_trunc('day',pickup_datetime) = '2021-02-15'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6540a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|trip_duration|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-------------+\n",
      "|           HV0005|              B02510|2021-02-11 13:40:44|2021-02-12 10:39:44|         247|          41|   null|        75540|\n",
      "|           HV0004|              B02800|2021-02-17 15:54:53|2021-02-18 07:48:34|         242|         254|   null|        57221|\n",
      "|           HV0004|              B02800|2021-02-20 12:08:15|2021-02-21 00:22:14|         188|          55|   null|        44039|\n",
      "|           HV0003|              B02864|2021-02-03 20:24:25|2021-02-04 07:41:58|          51|         147|   null|        40653|\n",
      "|           HV0003|              B02887|2021-02-19 23:17:44|2021-02-20 09:44:01|         210|         149|   null|        37577|\n",
      "|           HV0003|              B02764|2021-02-25 17:13:35|2021-02-26 02:57:05|         174|         126|   null|        35010|\n",
      "|           HV0003|              B02875|2021-02-20 01:36:13|2021-02-20 11:16:19|         242|          31|   null|        34806|\n",
      "|           HV0005|              B02510|2021-02-18 15:24:19|2021-02-19 01:01:11|         196|         197|   null|        34612|\n",
      "|           HV0003|              B02764|2021-02-18 01:31:20|2021-02-18 11:07:15|          89|         265|   null|        34555|\n",
      "|           HV0005|              B02510|2021-02-10 20:51:39|2021-02-11 06:21:08|         254|         259|   null|        34169|\n",
      "|           HV0003|              B02764|2021-02-10 01:56:17|2021-02-10 10:57:33|          61|         265|   null|        32476|\n",
      "|           HV0005|              B02510|2021-02-25 09:18:18|2021-02-25 18:18:57|         169|         265|   null|        32439|\n",
      "|           HV0005|              B02510|2021-02-21 19:59:13|2021-02-22 04:56:16|          10|          10|   null|        32223|\n",
      "|           HV0003|              B02864|2021-02-09 18:36:13|2021-02-10 03:31:00|          78|         147|   null|        32087|\n",
      "|           HV0004|              B02800|2021-02-06 09:48:09|2021-02-06 18:32:16|         229|         188|   null|        31447|\n",
      "|           HV0005|              B02510|2021-02-02 09:42:30|2021-02-02 18:17:43|          85|          85|   null|        30913|\n",
      "|           HV0005|              B02510|2021-02-10 10:12:08|2021-02-10 18:46:24|          29|         125|   null|        30856|\n",
      "|           HV0003|              B02764|2021-02-09 13:30:13|2021-02-09 22:02:25|         188|         265|   null|        30732|\n",
      "|           HV0005|              B02510|2021-02-21 22:50:52|2021-02-22 07:21:52|         177|          73|   null|        30660|\n",
      "|           HV0005|              B02510|2021-02-05 21:32:33|2021-02-06 06:01:04|          97|          72|   null|        30511|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_duration = spark.sql(\"\"\"\n",
    "SELECT \n",
    "        *,\n",
    "        unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime) AS trip_duration\n",
    "FROM fhvhv\n",
    "ORDER BY unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime) DESC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b134049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>  (190 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|dispatching_base_num|count(1)|\n",
      "+--------------------+--------+\n",
      "|              B03136|    1741|\n",
      "|              B02844|    3502|\n",
      "|              B02512|   41043|\n",
      "|              B02865|   76160|\n",
      "|              B02800|   84277|\n",
      "|              B02870|  101945|\n",
      "|              B02395|  112433|\n",
      "|              B02880|  115716|\n",
      "|              B02836|  128978|\n",
      "|              B02889|  138762|\n",
      "|              B02888|  169167|\n",
      "|              B02835|  189031|\n",
      "|              B02877|  198938|\n",
      "|              B02867|  200530|\n",
      "|              B02879|  210137|\n",
      "|              B02876|  215693|\n",
      "|              B02882|  232173|\n",
      "|              B02884|  244963|\n",
      "|              B02883|  251617|\n",
      "|              B02617|  274510|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trip_duration = spark.sql(\"\"\"\n",
    "SELECT dispatching_base_num, COUNT(*)\n",
    "FROM fhvhv\n",
    "GROUP BY dispatching_base_num\n",
    "ORDER BY COUNT(*)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327d26a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-01 16:51:50--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.137.80\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.137.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv.2’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-03-01 16:51:50 (55.2 MB/s) - ‘taxi+_zone_lookup.csv.2’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c94bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa5c560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a57ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup.registerTempTable('look_up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca78395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============================================>        (169 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|      pickup_dropoff|count(1)|\n",
      "+--------------------+--------+\n",
      "|East New York/Gov...|  135123|\n",
      "|Borough Park/Gove...|  111987|\n",
      "|East New York/Corona|   90082|\n",
      "|Canarsie/Governor...|   84078|\n",
      "|Crown Heights Nor...|   77928|\n",
      "| Borough Park/Corona|   74658|\n",
      "|     Canarsie/Corona|   56052|\n",
      "|Bay Ridge/Governo...|   53802|\n",
      "|Crown Heights Nor...|   51952|\n",
      "|East New York/Fla...|   45041|\n",
      "|East New York/Jam...|   45041|\n",
      "|East New York/Red...|   45041|\n",
      "|East New York/Cen...|   45041|\n",
      "|East New York/Lon...|   45041|\n",
      "|East New York/Rid...|   45041|\n",
      "|East New York/DUM...|   45041|\n",
      "|East New York/Gre...|   45041|\n",
      "|East New York/Wil...|   45041|\n",
      "|East New York/Spr...|   45041|\n",
      "|East New York/Mid...|   45041|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trip_1 = spark.sql(\"\"\"\n",
    "with tab_1 as (SELECT COALESCE(p.zone, 'Unknown') as pickup_zone, COALESCE(d.zone, 'Unknown') as dropoff_zone\n",
    "FROM fhvhv f\n",
    "INNER JOIN look_up p\n",
    "ON p.LocationID = f.PULocationID\n",
    "INNER JOIN look_up d\n",
    "ON p.LocationID = f.DOLocationID)\n",
    "SELECT CONCAT_WS('/',pickup_zone,dropoff_zone ) pickup_dropoff, count(*)\n",
    "FROM tab_1\n",
    "group by 1\n",
    "order by 2 desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
