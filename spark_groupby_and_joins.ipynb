{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b969e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344c8b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/GBENGA/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 15:11:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName('test') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7819ed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c899a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382033be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d9ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green.withColumn(\"lpep_pickup_datetime\", df_green[\"lpep_pickup_datetime\"].cast(types.TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324aa94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ---Revenue grouping\n",
    "    PULocationID AS zone,\n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour_month,\n",
    "    \n",
    "    SUM(total_amount) AS amount_,\n",
    "    COUNT(1) AS number_records\n",
    "FROM green\n",
    "GROUP BY 1,2\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c82e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+--------------+\n",
      "|zone|         hour_month|           amount_|number_records|\n",
      "+----+-------------------+------------------+--------------+\n",
      "| 129|2020-01-19 17:00:00|117.61999999999998|            10|\n",
      "|  33|2020-01-10 19:00:00|            456.18|            28|\n",
      "|  74|2020-01-15 02:00:00|             73.81|             6|\n",
      "| 166|2020-01-23 08:00:00|            544.23|            33|\n",
      "|  83|2020-01-04 02:00:00|56.260000000000005|             5|\n",
      "| 134|2020-01-15 08:00:00| 55.85999999999999|             4|\n",
      "|  81|2020-01-27 18:00:00|             90.86|             3|\n",
      "| 223|2020-01-13 09:00:00|             70.41|             5|\n",
      "| 196|2020-01-29 15:00:00|            289.47|            15|\n",
      "| 247|2020-01-30 15:00:00|43.900000000000006|             3|\n",
      "| 213|2020-01-05 09:00:00|206.48999999999998|             7|\n",
      "|  24|2020-01-07 05:00:00|              63.5|             4|\n",
      "| 244|2020-01-26 06:00:00|13.600000000000001|             2|\n",
      "| 244|2020-01-03 14:00:00| 660.1899999999999|            27|\n",
      "| 130|2020-01-15 12:00:00|            142.34|             6|\n",
      "|  14|2020-01-05 17:00:00|              22.6|             2|\n",
      "| 166|2020-01-24 20:00:00|            705.12|            44|\n",
      "|  74|2020-01-25 15:00:00|1360.7899999999984|            89|\n",
      "|  74|2020-01-02 10:00:00|            759.65|            65|\n",
      "|   7|2020-01-30 20:00:00|418.60000000000014|            33|\n",
      "+----+-------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5de40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue.write.parquet('data/report/revenue/green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d681290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')\n",
    "df_yellow.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aeb6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ---Revenue grouping\n",
    "    PULocationID AS zone,\n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour_month,\n",
    "    \n",
    "    SUM(total_amount) AS amount_,\n",
    "    COUNT(1) AS number_records\n",
    "FROM yellow\n",
    "WHERE tpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY 1,2\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e17b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+--------------+\n",
      "|zone|         hour_month|           amount_|number_records|\n",
      "+----+-------------------+------------------+--------------+\n",
      "|  43|2020-01-24 20:00:00|2786.8399999999974|           180|\n",
      "| 230|2020-01-29 18:00:00| 11976.85000000003|           664|\n",
      "| 163|2020-01-14 21:00:00| 6199.080000000006|           381|\n",
      "|  79|2020-01-01 00:00:00|12573.810000000034|           721|\n",
      "| 132|2020-01-12 20:00:00|31807.889999999992|           590|\n",
      "|  43|2020-01-24 08:00:00| 3534.329999999997|           232|\n",
      "| 162|2020-01-26 13:00:00| 4528.120000000001|           295|\n",
      "| 238|2020-01-08 08:00:00|5751.6500000000015|           348|\n",
      "| 249|2020-01-28 08:00:00| 2671.499999999998|           179|\n",
      "|  68|2020-01-09 23:00:00| 4352.069999999998|           231|\n",
      "| 263|2020-01-23 13:00:00| 2498.149999999998|           188|\n",
      "| 234|2020-01-15 23:00:00| 5139.520000000002|           308|\n",
      "|  90|2020-01-04 10:00:00| 2216.149999999998|           167|\n",
      "| 262|2020-01-15 11:00:00|2724.4599999999964|           184|\n",
      "| 114|2020-01-26 03:00:00| 4177.289999999997|           241|\n",
      "| 236|2020-01-21 12:00:00| 9486.900000000023|           691|\n",
      "| 138|2020-01-02 09:00:00| 10896.63000000001|           285|\n",
      "| 232|2020-01-18 20:00:00|271.41999999999996|            19|\n",
      "| 230|2020-01-08 19:00:00|10594.600000000031|           640|\n",
      "| 262|2020-01-07 10:00:00| 3741.489999999996|           231|\n",
      "+----+-------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40f1d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue.write.parquet('data/report/revenue/yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9e414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
